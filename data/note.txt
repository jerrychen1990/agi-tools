python的迭代器与生成器
* 实现了__next__(self)函数的可以被称为一个一个迭代器， 可以用next(it)的方式调用
* iter(x)方法调用对象x的__iter__(self)方法，返回的对象应该是一个迭代器。通常给collections添加此方法
* For循环调用的就是next方法，遇到StopIteration  异常结束循环
* 带有yield关键字的函数是一个生成器函数，返回的值是一个生成器。生成器本质上还是一个迭代器
* 用生成器函数的方式构建生成器（迭代器）要比直接构建迭代器优雅、方便

* tokenizer
    * 词粒度
        * 词表太大，oov
    * 字粒度
        * 词表小，但是单个字没有具体含义（特别是英文）
    * 子词粒度
        * 通过词频统计组合
            * BPE
            * UML
            * WordPiece（结合了BPE和UML）
    * sentence-piece
        * 不需要提前分词、可以引入空格信息
* 其他
    * Transformer-XL
        * 解决长文本的上下文关系问题
        * 之前的segment的参数被固定，放在一个类似cache的地方，可以被后文的attention看到
    * ELMO
        * 两个反向LSTM拼接而成，动态词向量
    * Word2Vec
        * 上下文信息表达当前word，静态词向量
    * Glove
        * Word2Vec的进化版
* Sentence Level
    * Sentence-bert
        * 孪生网络
        * bert过完句子，采用（avg，max-pool， cls）作为句子的embedding
        * 采用 triplet loss/regression/classification loss作为损失函数训练
    * bert-flow
        * BERT预训练出来的句子embedding不平滑，不能直接点乘来计算相似度
        * 训练一个normalizing flow，来把bert的embedding平滑，得到一个更好的表征句子相似度的embedding
* Encoder
    * 预训练任务
        * MLM （Denosing Autoencoder）
            * 下游任务：（分类，NER，纠错）
            * BERT
                * mask token
                * position embedding
            * Albert
                * 共享权重
                * embedding分解
                * nsp变成sentence ordering
                * 训练时间变快，inference没有变快，相同inference时间下效果不如bert
            * ELECTRA
                * 两个MLM，可以共享一部分权重
                * generator：类似bert的MLM
                * discriminator：判断generator生成出来的token是original还是replaced
* Decoder
    * XLNET
        * 结合BERT能看见上下文和LM的预训练、finetune任务一致的优势
        * 打乱原文（attention mask）的顺序，用LM预测最后的几个词
        * 引入了transformer XL
        * 阅读理解任务上相对于bert提升明显
* Encoder-Decoder
    * 预训练任务
        * Denoising 
            * 下游任务（翻译、摘要）
            * BART
                * seq2seq架构
                * 对原文加入噪音（mask， 打乱顺序，删除token等），decoder恢复出来
                * 较好的支持下游生成任务，同时也能学习到encoder各个位置的信息



* 判断一个人执行力强大与否，就是看他做的不好的时候能否坚持继续去做
* 一个人的发音与听力往往是没有直接联系的
* 当你专注时，时间会飞快流逝。所以专注时度过那个笨拙的起步过程的最好方法
* 个人商业模式
    * 一份时间出售一次：朝九晚五（有个很低的上限）
    * 一份时间出售多次：作者/内容创作
    * 购买别人的时间再出售：企业家、投资者
* 你见过多少人因为磨洋工和频繁跳槽获得财富自由了？
* 你的薪水只是你的估值，关注你的自身价值更重要
* 有能力的人最终都会被低估了，当自己被过分低估的时候，就是自己该出来闯荡一番的时候
* 给自己打工的同时给老板打工
* 必须做刚需产品？
* 中国人口基数大，才是中国书籍卖的便宜的原因。薄利多销
* 
* 
* 每天投资自己的时间有多少？怎么投资的，投资到哪些方向？
* 
* 注意力（流量）就是一种资源，有很多买卖注意力的商业模式（广告）。管理好自己的注意力就是管理好自己的资产
* 注意力基于时间，感觉两者没有特别大的区别（058）
* 能够长时间集中注意力几乎是所有学习能力强的人的必备标志
* TED
* 几乎所有进步都是在放弃了部分安全感的情况下获得的
* 不要和缺乏安全感的人合作（他们不会从内心里信任你）
* 
需要拓宽视野，对未来有个准确的预测
* 活在未来
    * 未来的世界是什么样的？（采集信息）
    * 未来的自己希望是什么样的？
    * 需要做哪些准备？
    * 准备（活在未来）
* 你自己得是一个贵人，才能遇到更多贵人
* 不要做表现型人格的人
* 真正的安全感来自于对于未来深刻全面的思考
* 抱怨会让一个人变得令人厌倦和讨厌
* 要了解一个人，看他的书单不失为一种好方法
* 成功无非是解答题高手作对了选择题
    * 要有判断的能力、要有执行力，缺一不可
* 价值观形成的过程中，着重需要注意的是避免以偏概全
* 正确选择“难受”就是正确选择刚需，驱动力
* 不在1%就是落后
* 每次跨界就是给自己扩展一个新的维度（多维度人才才有竞争力）
* 价格决定于供需，做被需要的人
* 换位思考，不是和某个人换，而是和整个世界换。考虑世界上需要什么样的人
* 我需要的跨界（金融、脑科、心理。。。）
* 镜像神经元https://www.zhihu.com/question/25674902
    * 看到别人的某项行为，自己能有相同的感受，心理学重要的依据
* 学习的一个方法：物理上尽可能接近成功的目标
* 表现型人格在意自己当下的表现
* 进取型人格在意自己未来的表现
* 当你想学习某项技能时，给他赋予一些重大的意义，让自己根本没法停下来（坚持、努力）都太被动、痛苦
* 在重复足够多次数（熟练到神经元内化）之前，很多人就放弃了，从而体会不到那种神经元内化之后熟练的快感
* 成功都是规避风险的，别人看起来他们是在冒险，其实他们根据自己的判断，做了很多风险规避的策略
* 判断趋势时，要看多个周期
        
* 

* 情绪是理智的快捷方式、直觉是情绪的快捷方式
* 财富自由只是一个里程碑，后面还有更长的路要走，不能和家人一起成长就很难走下去
